{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24bd548e-befc-4823-912e-ea6ff3cca959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4815a6d6-e7b6-48f8-84c5-e0ead3218733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader('ML.pdf')\n",
    "pages = pdf_loader.load_and_split()\n",
    "#print(pages[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80746e59-9c4b-4bce-bc4d-81e4ac36d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_llm_text = VertexAI(model_name=\"text-bison@001\", max_output_tokens = 1000)\n",
    "vertex_embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00fd384b-ce83-41c1-964d-321d5c7b50d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the document content.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\n",
    "context = \"\\n\\n\".join(str(p.page_content) for p in pages)\n",
    "texts = text_splitter.split_text(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44ddbb-ac85-43b2-9d55-cad7fddb53b5",
   "metadata": {},
   "source": [
    " Here, the system first calculates the similarity between the question and the vectors in the database. The most similar vectors are then used to fetch the context that is relevant to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a657a9ab-0062-47a0-aa71-d2e416e11bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then, create the similarity search index using Chroma\n",
    "vector_index = Chroma.from_texts(texts, vertex_embeddings).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0485712f-0ade-48f3-a7b0-2125f9e9ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"IMPACT OF ONLINE SALES ON COSTS AT NETFLIX?\"\n",
    "question = \"What is Delayed reward in Reinforcement learning ?\"\n",
    "#question = \"What is Postponement ? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa866ad3-613f-4799-afea-4d5168013fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, retrieve relevant context using the original question.\n",
    "docs = vector_index.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd02d361-8e8b-487b-a4b0-7f6046188ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f87a2ee3-d928-49c6-bde3-091b72992678",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \" \".join(str(e.page_content) for e in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4027d153-749b-44da-9ee9-e82cbd2225f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_rm_whitespaces(ip_str):\n",
    "    \n",
    "    ip_str = ip_str.replace(\"\\n\", \" \")\n",
    "    ip_str = ip_str.replace(\"\\\\n\", \"\")\n",
    "    ip_str = ip_str.replace(\"\\\\u\", \"\")\n",
    "    ip_str = ip_str.replace(\"(\", \"\")\n",
    "    ip_str = ip_str.replace(\")\", \"\")\n",
    "    ip_str = ip_str.replace(\"{\", \"\")\n",
    "    ip_str = ip_str.replace(\"}\", \"\")\n",
    "    ip_str = ip_str.replace(\":\", \"\")\n",
    "    ip_str = ip_str.replace(\"'\", \"\")\n",
    "    ip_str = ip_str.replace(\".\", \"\")\n",
    "    ip_str = ip_str.replace(\",\", \"\")\n",
    "    \n",
    "    while \"  \" in ip_str:\n",
    "        ip_str = ip_str.replace(\"  \", \" \")\n",
    "\n",
    "    return ip_str\n",
    "context = str_rm_whitespaces(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f3660eb-c583-480a-abf4-2b46c309d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5909\n"
     ]
    }
   ],
   "source": [
    "print(len(context.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fce5c6c-6252-4b61-84b0-a03934093af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt_template = \"\"\"\n",
    "                    Answer the question as precise as possible using the provided context. \\n\\n\n",
    "                    Context: \\n {context} \\n\n",
    "                    Question: \\n {question} \\n\n",
    "                    Answer:\n",
    "                    \"\"\"\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# summaries is required. a bit confusing.\n",
    "combine_prompt_template = \"\"\"Given the extracted content and the question, find answer in a docs and summerize answer in 1000 words.\n",
    "If the answer is not contained in the docs, say \"answer not available in context. \\n\\n\n",
    "Summaries: \\n {summaries}?\\n\n",
    "Question: \\n {question} \\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "334a06cf-30c3-40bc-891f-523432ce59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_chain = load_qa_chain(\n",
    "    vertex_llm_text,\n",
    "    chain_type=\"map_reduce\",\n",
    "    return_intermediate_steps=True,\n",
    "    question_prompt=question_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2413d7f8-4fb8-4710-8513-c43b17d32cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_embeddings_outputs = map_reduce_chain(\n",
    "    {\"input_documents\": docs, \"question\": question}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "994ff8db-ca35-4f0c-ac34-75caf3f7a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = str_rm_whitespaces(map_reduce_embeddings_outputs[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f053a6b3-86e8-4671-9b23-7010341c58e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Delayed reward is a reward that is given to an agent after a certain amount of time has passed This can be problematic for reinforcement learning algorithms as they may not be able to learn the correct behavior if the reward is delayed There are a number of ways to deal with delayed rewards such as using temporal-difference learning or using a reward shaping technique'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a758c5b5-603d-4d5c-8926-06c675804152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Delayed reward is a reward that is given to an agent after a certain amount of time has passed. This can be problematic for reinforcement learning algorithms, as they may not be able to learn the correct behavior if the reward is delayed. There are a number of ways to deal with delayed rewards, such as using temporal-difference learning or using a reward shaping technique.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce_embeddings_outputs[\"output_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f28e7-0d52-41fe-b67c-77dec47e7468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-env_preview_lc-py",
   "name": "common-cpu.m105",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m105"
  },
  "kernelspec": {
   "display_name": "Python [conda env:env_preview_lc]",
   "language": "python",
   "name": "conda-env-env_preview_lc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
